# -*- coding: utf-8 -*-
"""Banking_subscription_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dv0UdVBZ4IWKGuSW2A1rcvBbpc-3sMzu
"""

import pandas as pd 
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns
sns.set()

#create a dataframe out of the csv file 
df = pd.read_csv('/content/bank-additional-full.csv')

df.head()

print(df.columns.values)

print(list(df))

df.info()

#split the first column into many columns, separated by semi-colons
df[['age', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month','day_of_week', 'duration', 'campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed', 'y' ]] = df.iloc[:, 0].apply(lambda x: pd.Series(str(x).split(";")))
print(df)

df.head()

#delete the first column
del df[df.columns[0]]
print(df)

df.head()

"""Great, now we have the dataframe in a workable state... let's check it out """

df.info()

print(df['emp.var.rate'].unique())

print(df['cons.price.idx'].unique())

print(df['campaign'].unique())

print(df['month'].unique())

print(df['cons.conf.idx'].unique())

print(df['euribor3m'].unique())

print(df['nr.employed'].unique())

print(df['poutcome'].unique())

#convert age, duration, , to numeric columns
numerics = ["age","duration", 'pdays', 'euribor3m', 'campaign', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'nr.employed']
for col in numerics:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    
df.info()

#convert _job, marital, education,default, housing, loan, contact, month, day-of-week,_campaign, pdays, previous, emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed, y__ columns to categorical 
columns = ["job","marital",'education', 'default','housing','loan','contact', 'month', 'day_of_week','poutcome', 'y']
for col in columns:
    df[col] = df[col].astype('category')
df.info()

#Check for unique values in different columns
df.nunique()

#Check for null values in columns 
df.isna().sum()

"""No explicitly NaN values"""

#see if null values were showing up as " " as opposed to "NaN"
df.replace(' ', np.nan, inplace=True)
df.isna().sum()

"""Nope """

#check for duplicates 
sum(df.duplicated())

"""12 rows are complete duplicates """

#Drop duplicates 
df.drop_duplicates(inplace=True)

#Re check for duplicates 
sum(df.duplicated())

#check to see whether the basic statistics make sense 
df.describe().transpose()

# 0 values for 'previous' and 'pdays' could make sense... but maybe not 'duration column'.... check out 'duration' column 
df['duration'].describe()

"""4918s max duraiton seems too high ... drop any row that has a duration more than 1 hour """

#get the indexes of durations that are larger than 3600 
df[(df['duration'] > 3600)]['duration']

#remove these rows
df = df.drop([7727, 13820, 22192, 24091, 40537])

#Check that these rows are gone
df[(df['duration'] > 3600)]['duration']

"""##Visualize some of the data 

Bar chart: 
- % of yes and no's per month 
- ages (binned) vs. % 'Yes' and 'no's per bin 
  (0-18) 
- average duration of 'Yes' and 'no' 
- average campaign vs Yes and No

"""

#month vs. yes and no's 

sns.histplot(data=df,
            x=df['month'], 
             hue = df['y'])
            #y=df['y'])
            #hue=None, order=None, hue_order=None, estimator='mean', errorbar=('ci', 95), n_boot=1000, units=None, seed=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, errcolor='.26', errwidth=None, capsize=None, dodge=True, ci='deprecated', ax=None, **kwargs)

#Age vs yes and no 
sns.histplot(data=df,
            x=df['age'], 
             hue = df['y'])
            #y=df['y'])
            #hue=None, order=None, hue_order=None, estimator='mean', errorbar=('ci', 95), n_boot=1000, units=None, seed=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, errcolor='.26', errwidth=None, capsize=None, dodge=True, ci='deprecated', ax=None, **kwargs)

#duration vs. yes and nos
sns.histplot(data=df,
            x=df['duration'], 
             hue = df['y'])
            #y=df['y'])
            #hue=None, order=None, hue_order=None, estimator='mean', errorbar=('ci', 95), n_boot=1000, units=None, seed=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, errcolor='.26', errwidth=None, capsize=None, dodge=True, ci='deprecated', ax=None, **kwargs)

"""##One Hot Encode Categorical Variables

One hot encode some categorical variables: 
[["job","marital","education","default","housing","loan","contact,"month","day_of_week","poutcome"]]
"""

#categorical data
categorical_cols = ["job","marital","education","default","housing","loan","contact", "month","day_of_week","poutcome"] 

#import pandas as pd
df = pd.get_dummies(df, columns = categorical_cols)
df.head()

"""##Analysis 

We can try out this classification problem with the following techniques: 

* Logistic Regression
* Random Forests
* K-Nearest Neighbours
* Support Vector Machines
* Neural Networks





"""





"""### Logistic Regression

Create variables matrix and target vector
"""

df.columns.values

#Create feature and target matrices from the dataframe 

X = df[['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate',
       'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed',
       'job_"admin."', 'job_"blue-collar"', 'job_"entrepreneur"',
       'job_"housemaid"', 'job_"management"', 'job_"retired"',
       'job_"self-employed"', 'job_"services"', 'job_"student"',
       'job_"technician"', 'job_"unemployed"', 'job_"unknown"',
       'marital_"divorced"', 'marital_"married"', 'marital_"single"',
       'marital_"unknown"', 'education_"basic.4y"',
       'education_"basic.6y"', 'education_"basic.9y"',
       'education_"high.school"', 'education_"illiterate"',
       'education_"professional.course"', 'education_"university.degree"',
       'education_"unknown"', 'default_"no"', 'default_"unknown"',
       'default_"yes"', 'housing_"no"', 'housing_"unknown"',
       'housing_"yes"', 'loan_"no"', 'loan_"unknown"', 'loan_"yes"',
       'contact_"cellular"', 'contact_"telephone"', 'month_"apr"',
       'month_"aug"', 'month_"dec"', 'month_"jul"', 'month_"jun"',
       'month_"mar"', 'month_"may"', 'month_"nov"', 'month_"oct"',
       'month_"sep"', 'day_of_week_"fri"', 'day_of_week_"mon"',
       'day_of_week_"thu"', 'day_of_week_"tue"', 'day_of_week_"wed"',
       'poutcome_"failure"', 'poutcome_"nonexistent"',
       'poutcome_"success"']]


y = df['y']

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an instance of the LogisticRegression class
clf = LogisticRegression()

# Fit the model to the training data
clf.fit(X_train, y_train)

# Evaluate the model on the test data
accuracy = clf.score(X_test, y_test)
print(f'Test accuracy: {accuracy:.2f}')

"""91% accuracy with logistic regression """



"""###Random Forest classification 


"""

from sklearn.ensemble import RandomForestClassifier # for classification

clf = RandomForestClassifier(n_estimators=100) # create an instance of the classifier 
clf.fit(X_train, y_train) # fit the data 
y_pred = clf.predict(X_test) # predict on the test data

#Evaluation

from sklearn.metrics import accuracy_score, f1_score # accuracy and f1 

acc = accuracy_score(y_test, y_pred) #Accuracy 
f1 = f1_score(y_test, y_pred, pos_label='"yes"') #f1

print(acc, f1)

"""91.28% accuracy and .54 f1 score with Random Forest Classifier"""



"""###K Nearest Neighbors """

#Imports 
from sklearn.neighbors import KNeighborsClassifier

#create an instance of the classifier
knn = KNeighborsClassifier(n_neighbors=5)

#Fit the model
knn.fit(X_train, y_train)

#Predictions
y_pred = knn.predict(X_test)

#Evaluations 
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, pos_label='"yes"') #f1

print(acc, f1)

"""90% accuracy and .53 f1"""



"""###Support Vector Machine """

from sklearn.svm import SVC

#create an instance of the model
svc = SVC(kernel='linear') # for linear kernel

#fit model
svc.fit(X_train, y_train)

#predict with the model 
y_pred = svc.predict(X_test)

#Evaluations 

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, pos_label='"yes"') #f1

print(acc, f1)

"""90.9% accuracy, .49 f1"""



"""###Neural Networks (MLP) 

"""

from sklearn.neural_network import MLPClassifier
import tensorflow as tf

clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000)

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

#Evaluations 

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, pos_label='"yes"') #f1

print(acc, f1)

"""90% accuracy and .51 F1 score """